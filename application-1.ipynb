{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code example works with raw MS Data downloaded from the MetaboLights repository (~ 1 GiB). The running time for the code in this notebook is approximately 30 s (excluding the time required to download the data), using a personal computer with an 8th generation Intel i5 processor and 8 GiB memory.**\n",
    "\n",
    "# Application 1: System suitability check, and signal drift evaluation\n",
    "\n",
    "This notebook introduces the analysis of the application #1 published in [Metabolites](https://doi.org/10.3390/metabo10100416). It shows how to work with raw data using as an example a System Suitability Check conducted in a metabolomics experiment: System Suitability Samples (SSS) were prepared using five known chemical standards:\n",
    "\n",
    "* Alogliptin\n",
    "* Phe-Phe\n",
    "* Tryptophan\n",
    "* LPC 18:0\n",
    "* Leu-Enk\n",
    "\n",
    "Ten SSS samples (addressed as SSS1) were consecutively run and used to build an acceptance criteria, and then compared against values obtained from the analysis of SSS samples that were analyzed before (SSS2) and after (SSS3) the study samples. This analysis is displayed in Figure 3.\n",
    "\n",
    "A similar analysis was conducted using QC samples that were spiked with the same compounds and with Leu-13C used as internal standard, but in this case, no acceptance criteria was defined. These results are displayed in Figure S1.\n",
    "\n",
    "\n",
    "<img src=\"fig/sample-list.png\" width=700>\n",
    "Sample list used in the experiment. The first SSS sample is the SSS2 and the second SSS sample is SSS3. SSS1 is not shown in this figure.\n",
    "\n",
    "**UPDATE 2022-06**: The code has been modified to work with the Assay object. See [this link](agregar) for a description on how to work with Assay objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidyms as ms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from download_from_metabolights import get_application1_data\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "import bokeh.plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from Metabolights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the raw data path and a DataFrame with sample metadata\n",
    "data_path = \"data\"\n",
    "get_application1_data(data_path)  # download data from metabolights\n",
    "sample_list_path = os.path.join(data_path, \"sample_list.csv\")\n",
    "centroid_data_path = os.path.join(data_path, \"cent\")\n",
    "sample_list = pd.read_csv(sample_list_path)\n",
    "# sample_list = sample_list[~(sample_list[\"class\"] == \"SCQC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compounds used for SSS check, with their m/z and expected retention times\n",
    "d = {\n",
    "    \"Compound\": [\"Leu-13C\", \"Trp\", \"Phe-Phe\", \"Alogliptin\", \"LPC 18:0\", \"Leu-Enk\"],\n",
    "     \"rt\": np.array([75, 129, 320, 291, 775, 372]),\n",
    "     \"mz\": np.array([133.1056, 205.0977,313.1552, 340.1773, 524.3716, 556.2771])\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create assay\n",
    "assay = ms.Assay(assay_path=\"sss\", data_path=\"data/cent\", sample_metadata=sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# ROI creation\n",
    "mz = np.array([133.1056, 205.0977,313.1552, 340.1773, 524.3716, 556.2771])\n",
    "roi_params = {\"targeted_mz\": d[\"mz\"], \"min_intensity\": 500, \"tolerance\": 0.015}\n",
    "assay.detect_features(n_jobs=-1, **roi_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak detection\n",
    "assay.extract_features(n_jobs=-1, store_smoothed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak descriptors\n",
    "assay.describe_features(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build feature table\n",
    "assay.build_feature_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay.feature_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature correspondence\n",
    "assay.match_features(include_classes=[\"QC\"], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = assay.make_data_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unwanted features using the Retention times from the standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the compounds used in the SSS using the expected rt \n",
    "ft_to_compound = dict()\n",
    "for index in df.index:\n",
    "    mz = df.at[index, \"mz\"]\n",
    "    rt = df.at[index, \"rt\"]\n",
    "    compound = df.loc[index, \"Compound\"]\n",
    "    ft = data.select_features(mz, rt)\n",
    "    ftid = int(ft[0].split(\"-\")[-1])\n",
    "    ft_to_compound[ftid] = compound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3: m/z, Rt and area dispersion for SSS samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table = assay.feature_table.copy()\n",
    "feature_table[\"Compound\"] = feature_table[\"cluster_\"].map(ft_to_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE 3: \n",
    "\n",
    "sss_mask = feature_table[\"class_\"].isin([\"SSS1\", \"SSS2\", \"SSS3\"])\n",
    "sss_data = feature_table[sss_mask].copy()\n",
    "# sss_data = sss_data[sss_data[\"Compound\"] != 'LPC 18:0']\n",
    "\n",
    "# compute mean centered m/z and rt\n",
    "mean_mz = sss_data[\"mz\"].groupby(sss_data[\"Compound\"]).mean()\n",
    "mean_rt = sss_data[\"rt\"].groupby(sss_data[\"Compound\"]).mean()\n",
    "sss_data[\"mean mz\"] = \\\n",
    "    (sss_data[\"mz\"].groupby(sss_data[\"Compound\"])\n",
    "     .apply(lambda x: x - mean_mz[x.name]))\n",
    "\n",
    "sss_data[\"mean rt\"] = \\\n",
    "    (sss_data[\"rt\"].groupby(sss_data[\"Compound\"])\n",
    "     .apply(lambda x: x - mean_rt[x.name]))\n",
    "\n",
    "xvars = [\"mean mz\", \"mean rt\", \"area\"]\n",
    "g = sns.PairGrid(data=sss_data,\n",
    "                 y_vars=[\"Compound\"],\n",
    "                 x_vars=xvars,\n",
    "                 hue=\"class_\",\n",
    "                 hue_kws={\"marker\": [\".\", \"X\", \"D\"], \"size\": [8, 8, 8]},\n",
    "                 height=4)\n",
    "g.map(sns.stripplot)\n",
    "\n",
    "# setting plot properties\n",
    "g.axes[0, 0].set_xlim(-0.01, 0.01)\n",
    "g.axes[0, 2].set_xticks(np.linspace(0, 2e5, 5))\n",
    "t = g.axes[0, 2].get_xticks()\n",
    "t = [str(x / 100000) for x in t ]\n",
    "g.axes[0, 2].set_xticklabels(t);\n",
    "g.axes[0, 0].set_xlabel(\"Mean centered m/z\")\n",
    "g.axes[0, 1].set_xlabel(\"Mean centered Rt [s]\")\n",
    "g.axes[0, 2].set_xlabel(\"Area / $10^{5}$ [au]\");\n",
    "# g.savefig(\"metabolomics-2020-sss.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Figure S2: m/z, Rt and area dispersion for QC samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIGURE S1: \n",
    "\n",
    "# also remove LPC 18:0 because the area has much higher values\n",
    "qc_mask = feature_table[\"class_\"].isin([\"QC\"]) & (feature_table[\"Compound\"] != \"LPC 18:0\")\n",
    "qc_data = feature_table[qc_mask].copy()\n",
    "\n",
    "# compute mean centered m/z and rt\n",
    "mean_mz = qc_data[\"mz\"].groupby(qc_data[\"cluster_\"]).mean()\n",
    "mean_rt = qc_data[\"rt\"].groupby(qc_data[\"cluster_\"]).mean()\n",
    "qc_data[\"mean_mz\"] = \\\n",
    "    (qc_data[\"mz\"].groupby(qc_data[\"cluster_\"])\n",
    "     .apply(lambda x: x - mean_mz[x.name]))\n",
    "\n",
    "qc_data[\"mean_rt\"] = \\\n",
    "    (qc_data[\"rt\"].groupby(qc_data[\"cluster_\"])\n",
    "     .apply(lambda x: x - mean_rt[x.name]))\n",
    "# sss_data = sss_data.rename(columns={\"area\": \"Area / 10e5 [au]\"})\n",
    "\n",
    "xvars = [\"mean_mz\", \"mean_rt\", \"area\"]\n",
    "g = sns.PairGrid(data=qc_data,\n",
    "                 y_vars=[\"Compound\"],\n",
    "                 x_vars=xvars,\n",
    "                 hue=\"class_\",\n",
    "                 height=4)\n",
    "g.map(sns.stripplot)\n",
    "g.axes[0, 0].set_xlim(-0.01, 0.01)\n",
    "g.axes[0, 1].set_xlim(-1, 1)\n",
    "g.axes[0, 2].set_xticks(np.linspace(0, 3e5, 6))\n",
    "\n",
    "t = g.axes[0, 2].get_xticks()\n",
    "t = [str(x / 100000) for x in t ]\n",
    "g.axes[0, 2].set_xticklabels(t);g.axes[0, 0].set_xlabel(\"Mean centered m/z\")\n",
    "g.axes[0, 1].set_xlabel(\"Mean centered Rt [s]\")\n",
    "g.axes[0, 2].set_xlabel(\"Area / $10^{5}$ [au]\")\n",
    "# g.savefig(\"rt_mz_area_qc_lpc.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
